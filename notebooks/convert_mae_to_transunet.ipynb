{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6136c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mae.models_vit import vit_base_patch16\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.utils import _pair\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9f7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/home/lili/code/ssl/ssl-medical-sattelite/mae/mae_baseline_medical/checkpoint-49.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10897d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_model = checkpoint['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14e5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mae = vit_base_patch16()\n",
    "msg = model_mae.load_state_dict(checkpoint_model, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0b1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "\n",
    "    config.classifier = 'seg'\n",
    "    config.representation_size = None\n",
    "    config.resnet_pretrained_path = None\n",
    "    # config.pretrained_path = '../model/vit_checkpoint/imagenet21k/ViT-B_16.npz'\n",
    "    config.pretrained_path = '/home/lili/code/ssl/ssl-medical-sattelite/mae/mae_baseline_medical/checkpoint-49.pth'\n",
    "    config.patch_size = 16\n",
    "\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.n_classes = 2\n",
    "    config.activation = 'softmax'\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c47a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "#         self.decoder = DecoderCup(config)\n",
    "#         self.segmentation_head = SegmentationHead(\n",
    "#             in_channels=config['decoder_channels'][-1],\n",
    "#             out_channels=config['n_classes'],\n",
    "#             kernel_size=3,\n",
    "#         )\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n",
    "        x = self.decoder(x, features)\n",
    "        logits = self.segmentation_head(x)\n",
    "        return logits\n",
    "\n",
    "    def load_from_mae(self, weights):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            res_weight = weights\n",
    "            \n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(weights[\"patch_embed.proj.weight\"])\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(weights[\"patch_embed.proj.bias\"])\n",
    "\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(weights[\"norm.weight\"])\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(weights[\"norm.weight\"])\n",
    "\n",
    "            posemb = weights[\"pos_embed\"]\n",
    "\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "        \n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            elif posemb.size()[1]-1 == posemb_new.size()[1]:\n",
    "                posemb = posemb[:, 1:]\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
    "                ntok_new = posemb_new.size(1)\n",
    "                if self.classifier == \"seg\":\n",
    "                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = posemb_grid\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            # Encoder whole\n",
    "            for bname, block in self.transformer.encoder.named_children(): \n",
    "              \n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from_mae(weights, n_block=uname)\n",
    "            \n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    print(bname, block )\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(res_weight, n_block=bname, n_unit=uname)\n",
    "                        \n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)                        \n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89a7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        self.config = config\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:   # ResNet\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n",
    "            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  \n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.hidden_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hybrid:\n",
    "            x, features = self.hybrid_model(x)\n",
    "        else:\n",
    "            features = None\n",
    "        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings, features\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "    def load_from_mae(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            import pdb \n",
    "            \n",
    "#             'blocks.0.norm1.weight'   'blocks.0.attn.qkv.weight'  'blocks.0.attn.proj.weight' 'blocks.0.norm2.\n",
    "# 'blocks.0.mlp.fc1.w' 'blocks.0.mlp.fc2.weight'\n",
    "            \n",
    "            query_weight = weights[f'blocks.{n_block}.attn.qkv.weight'][:self.hidden_size] \n",
    "            key_weight = weights[f'blocks.{n_block}.attn.qkv.weight'][self.hidden_size: 2*self.hidden_size] \n",
    "            value_weight = weights[f'blocks.{n_block}.attn.qkv.weight'][2*self.hidden_size:] \n",
    "            \n",
    "            \n",
    "            out_weight = weights[f'blocks.{n_block}.attn.proj.weight']\n",
    "\n",
    "            query_bias = weights[f'blocks.{n_block}.attn.qkv.bias'][:self.hidden_size] \n",
    "            key_bias =  weights[f'blocks.{n_block}.attn.qkv.bias'][self.hidden_size: 2*self.hidden_size] \n",
    "            value_bias =  weights[f'blocks.{n_block}.attn.qkv.bias'][2*self.hidden_size:] \n",
    "            \n",
    "            out_bias = weights[f'blocks.{n_block}.attn.proj.bias']\n",
    "            \n",
    "            assert self.attn.query.weight.shape == query_weight.shape\n",
    "            assert self.attn.key.weight.shape == key_weight.shape\n",
    "            assert self.attn.value.weight.shape == value_weight.shape\n",
    "            assert self.attn.out.weight.shape == out_weight.shape\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            \n",
    "            \n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = weights[f'blocks.{n_block}.mlp.fc1.weight']\n",
    "            mlp_weight_1 = weights[f'blocks.{n_block}.mlp.fc2.weight']\n",
    "            mlp_bias_0 = weights[f'blocks.{n_block}.mlp.fc1.bias']\n",
    "            mlp_bias_1 = weights[f'blocks.{n_block}.mlp.fc2.bias']\n",
    "\n",
    "            assert self.ffn.fc1.weight.shape == mlp_weight_0.shape\n",
    "            assert self.ffn.fc2.weight.shape == mlp_weight_1.shape\n",
    "            assert self.ffn.fc1.bias.shape == mlp_bias_0.shape\n",
    "            assert self.ffn.fc2.bias.shape == mlp_bias_1.shape\n",
    "            \n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(weights[f'blocks.{n_block}.norm1.weight'])\n",
    "            self.attention_norm.bias.copy_(weights[f'blocks.{n_block}.norm1.bias'])\n",
    "            self.ffn_norm.weight.copy_(weights[f'blocks.{n_block}.norm2.weight'])\n",
    "            self.ffn_norm.bias.copy_(weights[f'blocks.{n_block}.norm2.bias'])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "    \n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        # encoded = self.encoder_norm(hidden_states)\n",
    "        return hidden_states, attn_weights\n",
    "        return encoded, attn_weights\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output, features = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n",
    "        return encoded, attn_weights, features\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd823a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'blocks.0.norm1.weight'   'blocks.0.attn.qkv.weight'  'blocks.0.attn.proj.weight' 'blocks.0.norm2.\n",
    "# 'blocks.0.mlp.fc1.w' 'blocks.0.mlp.fc2.weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee686578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "layer 1\n",
      "layer 2\n",
      "layer 3\n",
      "layer 4\n",
      "layer 5\n",
      "layer 6\n",
      "layer 7\n",
      "layer 8\n",
      "layer 9\n",
      "layer 10\n",
      "layer 11\n"
     ]
    }
   ],
   "source": [
    "net = VisionTransformer(get_b16_config(), img_size=224, num_classes=14)\n",
    "net.load_from_mae(checkpoint_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b90b86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mae.eval()\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78913398",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "input_1 = torch.Tensor(np.random.normal(size=(1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b88ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mae = model_mae.forward_features_transfer(input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28dc4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_copy = net.transformer(input_1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31fcd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c868922",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_almost_equal(res_mae.detach().numpy(), res_copy.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e46dc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.28922   ,  0.8574274 , -1.8156488 , ..., -2.8316395 ,\n",
       "         -1.2804598 ,  0.3285563 ],\n",
       "        [ 1.9431055 ,  5.518027  , -2.3719282 , ..., -4.5553746 ,\n",
       "         -1.151378  ,  1.2527792 ],\n",
       "        [ 3.1565354 ,  2.9455905 , -0.35940456, ..., -0.61962926,\n",
       "          0.06183657,  2.8153422 ],\n",
       "        ...,\n",
       "        [-2.5021086 , -0.01082426, -6.700266  , ..., -1.0508425 ,\n",
       "          3.5431    , -3.3149319 ],\n",
       "        [-1.0556322 ,  1.2118303 , -8.897961  , ..., -2.5832372 ,\n",
       "          4.1201477 , -1.0326865 ],\n",
       "        [ 4.4617686 ,  2.1963015 , -5.563488  , ..., -5.1784725 ,\n",
       "          0.46552312, -1.8062646 ]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " res_copy.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e791244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.28922   ,  0.8574274 , -1.8156488 , ..., -2.8316395 ,\n",
       "         -1.2804598 ,  0.3285563 ],\n",
       "        [ 1.9431055 ,  5.518027  , -2.3719282 , ..., -4.5553746 ,\n",
       "         -1.151378  ,  1.2527792 ],\n",
       "        [ 3.1565354 ,  2.9455905 , -0.35940456, ..., -0.61962926,\n",
       "          0.06183657,  2.8153422 ],\n",
       "        ...,\n",
       "        [-2.5021086 , -0.01082426, -6.700266  , ..., -1.0508425 ,\n",
       "          3.5431    , -3.3149319 ],\n",
       "        [-1.0556322 ,  1.2118303 , -8.897961  , ..., -2.5832372 ,\n",
       "          4.1201477 , -1.0326865 ],\n",
       "        [ 4.4617686 ,  2.1963015 , -5.563488  , ..., -5.1784725 ,\n",
       "          0.46552312, -1.8062646 ]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_mae.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fe0ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2 = torch.Tensor(np.random.normal(size=(1, 3, 224, 224)))\n",
    "res_mae_2 = model_mae.forward_features_transfer(input_2)\n",
    "res_copy_2 = net.transformer(input_2)[0]\n",
    "assert_almost_equal(res_mae_2.detach().numpy(), res_copy_2.detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
